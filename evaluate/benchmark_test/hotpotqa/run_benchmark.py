#!/usr/bin/env python3
"""
HotpotQA ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë¡œ í‰ê°€ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í¸ì˜ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from evaluate.benchmark_test.hotpotqa.evaluator import HotpotQAEvaluator


def run_quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10ê°œ ì§ˆë¬¸)"""
    print("ğŸš€ Running Quick Test (10 questions)")
    print("=" * 50)
    
    evaluator = HotpotQAEvaluator(
        max_samples=10,
        parallel_workers=1
    )
    
    return evaluator.run_evaluation()


def run_small_benchmark():
    """ì†Œê·œëª¨ ë²¤ì¹˜ë§ˆí¬ (50ê°œ ì§ˆë¬¸)"""
    print("ğŸš€ Running Small Benchmark (50 questions)")
    print("=" * 50)
    
    evaluator = HotpotQAEvaluator(
        max_samples=50,
        parallel_workers=2
    )
    
    return evaluator.run_evaluation()


def run_medium_benchmark():
    """ì¤‘ê°„ ê·œëª¨ ë²¤ì¹˜ë§ˆí¬ (200ê°œ ì§ˆë¬¸)"""
    print("ğŸš€ Running Medium Benchmark (200 questions)")
    print("=" * 50)
    
    evaluator = HotpotQAEvaluator(
        max_samples=200,
        parallel_workers=3
    )
    
    return evaluator.run_evaluation()


def run_full_benchmark():
    """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ (ëª¨ë“  ì§ˆë¬¸)"""
    print("ğŸš€ Running Full Benchmark (all questions)")
    print("=" * 50)
    print("âš ï¸  This may take several hours to complete!")
    
    confirm = input("Are you sure you want to run the full benchmark? (y/N): ")
    if confirm.lower() != 'y':
        print("Full benchmark cancelled.")
        return None
    
    evaluator = HotpotQAEvaluator(
        max_samples=None,  # ì „ì²´ ë°ì´í„°ì…‹
        parallel_workers=3
    )
    
    return evaluator.run_evaluation()


def run_single_method_test(method: str, num_questions: int = 20):
    """ë‹¨ì¼ ë°©ë²•ë¡  í…ŒìŠ¤íŠ¸"""
    print(f"ğŸš€ Running {method.upper()} Method Test ({num_questions} questions)")
    print("=" * 50)
    
    evaluator = HotpotQAEvaluator(
        max_samples=num_questions,
        parallel_workers=1
    )
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = evaluator.load_dataset()
    
    # ë‹¨ì¼ ë°©ë²•ë¡  í‰ê°€
    results, stats = evaluator.evaluate_reasoning_type(dataset, method)
    
    # ê²°ê³¼ ì¶œë ¥
    evaluator._print_stats(stats)
    
    return {method: stats}


def compare_methods_quick():
    """ë°©ë²•ë¡  ê°„ ë¹ ë¥¸ ë¹„êµ (ê°ê° 10ê°œ ì§ˆë¬¸)"""
    print("ğŸš€ Quick Methods Comparison (10 questions each)")
    print("=" * 50)
    
    results = {}
    
    for method in ["cot", "react", "reflexion"]:
        print(f"\nTesting {method.upper()}...")
        method_result = run_single_method_test(method, 10)
        results.update(method_result)
    
    # ë¹„êµ ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print("COMPARISON RESULTS")
    print(f"{'='*60}")
    
    print(f"{'Method':<12} {'Accuracy':<10} {'Avg Time':<10}")
    print("-" * 35)
    
    for method, stats in results.items():
        print(f"{method.upper():<12} {stats.accuracy:<10.3f} {stats.avg_response_time:<10.2f}s")
    
    return results


def interactive_menu():
    """ëŒ€í™”í˜• ë©”ë‰´"""
    while True:
        print("\nğŸ§ª HotpotQA Benchmark Evaluation")
        print("=" * 40)
        print("1. Quick Test (10 questions)")
        print("2. Small Benchmark (50 questions)")
        print("3. Medium Benchmark (200 questions)")  
        print("4. Full Benchmark (all questions)")
        print("5. Quick Methods Comparison")
        print("6. Test Single Method")
        print("7. Exit")
        
        try:
            choice = input("\nSelect an option (1-7): ").strip()
            
            if choice == "1":
                run_quick_test()
            elif choice == "2":
                run_small_benchmark()
            elif choice == "3":
                run_medium_benchmark()
            elif choice == "4":
                run_full_benchmark()
            elif choice == "5":
                compare_methods_quick()
            elif choice == "6":
                print("\nAvailable methods: cot, react, reflexion")
                method = input("Enter method name: ").strip().lower()
                if method in ["cot", "react", "reflexion"]:
                    num_q = input("Number of questions (default 20): ").strip()
                    num_questions = int(num_q) if num_q.isdigit() else 20
                    run_single_method_test(method, num_questions)
                else:
                    print("Invalid method name!")
            elif choice == "7":
                print("Goodbye!")
                break
            else:
                print("Invalid option. Please choose 1-7.")
                
        except KeyboardInterrupt:
            print("\n\nExiting...")
            break
        except Exception as e:
            print(f"Error: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="HotpotQA Benchmark Runner")
    parser.add_argument("--mode", choices=["quick", "small", "medium", "full", "compare", "interactive"], 
                       default="interactive", help="Evaluation mode")
    parser.add_argument("--method", choices=["cot", "react", "reflexion"], 
                       help="Single method to test")
    parser.add_argument("--questions", type=int, default=20, 
                       help="Number of questions for single method test")
    
    args = parser.parse_args()
    
    try:
        if args.mode == "quick":
            run_quick_test()
        elif args.mode == "small":
            run_small_benchmark()
        elif args.mode == "medium":
            run_medium_benchmark()
        elif args.mode == "full":
            run_full_benchmark()
        elif args.mode == "compare":
            compare_methods_quick()
        elif args.mode == "interactive":
            interactive_menu()
        
        if args.method:
            run_single_method_test(args.method, args.questions)
            
    except KeyboardInterrupt:
        print("\nEvaluation interrupted by user")
    except Exception as e:
        print(f"Error during evaluation: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
