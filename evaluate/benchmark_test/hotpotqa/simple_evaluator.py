#!/usr/bin/env python3
"""
HotpotQA í‰ê°€ê¸° ë…ë¦½ ì‹¤í–‰ ë²„ì „
import ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì½”ë“œë¥¼ ì§ì ‘ í¬í•¨
"""

import json
import os
import time
import logging
import importlib.util
import random
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed


@dataclass
class EvaluationResult:
    """í‰ê°€ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    reasoning_type: str
    question: str
    expected_answer: str
    predicted_answer: str
    is_correct: bool
    response_time: float
    question_type: str
    error: Optional[str] = None


@dataclass
class BenchmarkStats:
    """ë²¤ì¹˜ë§ˆí¬ í†µê³„ ë°ì´í„° í´ë˜ìŠ¤"""
    reasoning_type: str
    total_questions: int
    correct_answers: int
    accuracy: float
    avg_response_time: float
    bridge_accuracy: float
    comparison_accuracy: float
    bridge_count: int
    comparison_count: int


class SimpleHotpotQAEvaluator:
    """ë‹¨ìˆœí™”ëœ HotpotQA í‰ê°€ê¸°"""
    
    def __init__(
        self,
        data_path: Optional[str] = None,
        model: str = "openai:gpt-4o-mini",
        max_samples: Optional[int] = None,
        parallel_workers: int = 1,  # ì•ˆì •ì„±ì„ ìœ„í•´ 1ë¡œ ì„¤ì •
        timestamp: Optional[str] = None,
        seed: int = 42
    ):
        self.project_root = Path(__file__).parent.parent.parent.parent
        self.data_path = data_path or str(self.project_root / "raw_data" / "hotpot_dev_v1_simplified.json")
        self.model = model
        self.max_samples = max_samples
        self.parallel_workers = parallel_workers
        self.seed = seed
        
        # timestamp ì„¤ì • (ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ í˜„ì¬ ì‹œê°„ ì‚¬ìš©)
        self.timestamp = timestamp or time.strftime("%Y%m%d_%H%M%S")
        
        # seed ê³ ì •
        random.seed(self.seed)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.results_dir = Path(__file__).parent / "results"
        self.results_dir.mkdir(exist_ok=True)
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ íŒŒì¼ ê²½ë¡œë“¤
        self._setup_result_files()
        
        # ì—ì´ì „íŠ¸ ëª¨ë“ˆ ë¡œë“œ
        self._load_agent_module()
        self._initialize_agents()
    
    def _load_agent_module(self):
        """ì—ì´ì „íŠ¸ ëª¨ë“ˆ ì§ì ‘ ë¡œë“œ"""
        agent_path = self.project_root / "graphs" / "agent" / "experiments" / "qa_agent.py"
        spec = importlib.util.spec_from_file_location("qa_agent", agent_path)
        if spec is None or spec.loader is None:
            raise ImportError(f"Could not load agent module from {agent_path}")
        
        self.agent_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(self.agent_module)
        self.logger.info("Agent module loaded successfully")
    
    def _initialize_agents(self):
        """QA ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™”"""
        try:
            self.agents = {
                "cot": self.agent_module.create_cot_qa_agent(model=self.model, benchmark="hotpotqa"),
                "react": self.agent_module.create_react_qa_agent(model=self.model, benchmark="hotpotqa"),
                "reflexion": self.agent_module.create_reflexion_qa_agent(model=self.model, benchmark="hotpotqa")
            }
            self.logger.info("All QA agents initialized successfully")
        except Exception as e:
            self.logger.error(f"Error initializing agents: {e}")
            raise
    
    def load_dataset(self) -> List[Dict[str, Any]]:
        """HotpotQA ë°ì´í„°ì…‹ ë¡œë“œ"""
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # seedë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ìƒ˜í”Œë§ ë³´ì¥
            if self.max_samples and len(data) > self.max_samples:
                random.seed(self.seed)  # ìƒ˜í”Œë§ ì „ì— ë‹¤ì‹œ seed ì„¤ì •
                data = random.sample(data, self.max_samples)
                # ìƒ˜í”Œë§ í›„ ìˆœì„œë¥¼ ì¼ì •í•˜ê²Œ ìœ ì§€
                data.sort(key=lambda x: x.get('_id', ''))
            
            self.logger.info(f"Loaded {len(data)} questions from HotpotQA dataset")
            return data
        except Exception as e:
            self.logger.error(f"Error loading dataset: {e}")
            raise
    
    def normalize_answer(self, answer: str) -> str:
        """ë‹µë³€ ì •ê·œí™”"""
        import re
        import string
        
        answer = answer.lower()
        answer = re.sub(r'[{}]'.format(re.escape(string.punctuation)), ' ', answer)
        answer = re.sub(r'\s+', ' ', answer)
        answer = answer.strip()
        
        return answer
    
    def evaluate_answer(self, predicted: str, expected: str) -> bool:
        """ë‹µë³€ ì •í™•ì„± í‰ê°€"""
        pred_normalized = self.normalize_answer(predicted)
        exp_normalized = self.normalize_answer(expected)
        
        if pred_normalized == exp_normalized:
            return True
        
        if exp_normalized in pred_normalized:
            return True
        
        if exp_normalized in ["yes", "no"]:
            if exp_normalized in pred_normalized:
                return True
        
        return False
    
    def extract_final_answer(self, response: str, question_type: str) -> str:
        """ì‘ë‹µì—ì„œ ìµœì¢… ë‹µë³€ ì¶”ì¶œ"""
        import re
        
        if not isinstance(response, str):
            return str(response)
        
        # ë‹µë³€ íŒ¨í„´ ì°¾ê¸°
        answer_patterns = [
            r"(?i)final answer[:\s]*(.+?)(?:\n|$)",
            r"(?i)answer[:\s]*(.+?)(?:\n|$)", 
            r"(?i)the answer is[:\s]*(.+?)(?:\n|$)",
            r"(?i)conclusion[:\s]*(.+?)(?:\n|$)"
        ]
        
        for pattern in answer_patterns:
            match = re.search(pattern, response)
            if match:
                answer = match.group(1).strip()
                answer = re.sub(r'[.!?]+$', '', answer)
                return answer
        
        # Yes/No ì§ˆë¬¸ ì²˜ë¦¬
        if question_type == "comparison":
            yes_patterns = r"(?i)\b(yes|true|correct|same)\b"
            no_patterns = r"(?i)\b(no|false|incorrect|different)\b"
            
            if re.search(yes_patterns, response):
                return "yes"
            elif re.search(no_patterns, response):
                return "no"
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ ë°˜í™˜
        sentences = response.split('\n')
        for sentence in reversed(sentences):
            sentence = sentence.strip()
            if sentence and not sentence.startswith('**') and len(sentence) > 3:
                return sentence
        
        return response.strip()
    
    def evaluate_single_question(
        self, 
        question_data: Dict[str, Any], 
        reasoning_type: str
    ) -> EvaluationResult:
        """ë‹¨ì¼ ì§ˆë¬¸ í‰ê°€"""
        question = question_data["question"]
        expected_answer = question_data["answer"]
        question_type = question_data["type"]
        
        try:
            start_time = time.time()
            
            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            agent = self.agents[reasoning_type]
            result = agent.invoke({
                "messages": [{"role": "user", "content": question}]
            })
            
            response_time = time.time() - start_time
            
            # ì‘ë‹µ ì¶”ì¶œ
            if result and "messages" in result and result["messages"]:
                last_message = result["messages"][-1]
                if hasattr(last_message, 'content'):
                    response_content = last_message.content
                else:
                    response_content = str(last_message)
            else:
                response_content = "No response"
            
            predicted_answer = self.extract_final_answer(response_content, question_type)
            is_correct = self.evaluate_answer(predicted_answer, expected_answer)
            
            return EvaluationResult(
                reasoning_type=reasoning_type,
                question=question,
                expected_answer=expected_answer,
                predicted_answer=predicted_answer,
                is_correct=is_correct,
                response_time=response_time,
                question_type=question_type
            )
            
        except Exception as e:
            self.logger.error(f"Error evaluating question: {e}")
            return EvaluationResult(
                reasoning_type=reasoning_type,
                question=question,
                expected_answer=expected_answer,
                predicted_answer="ERROR",
                is_correct=False,
                response_time=0.0,
                question_type=question_type,
                error=str(e)
            )
    
    def evaluate_reasoning_type(
        self, 
        dataset: List[Dict[str, Any]], 
        reasoning_type: str
    ) -> Tuple[List[EvaluationResult], BenchmarkStats]:
        """íŠ¹ì • ì¶”ë¡  ë°©ë²•ë¡  í‰ê°€"""
        self.logger.info(f"Starting evaluation with {reasoning_type.upper()} reasoning...")
        
        results = []
        
        for i, question_data in enumerate(dataset):
            # ì´ë¯¸ ì™„ë£Œëœ ì§ˆë¬¸ì¸ì§€ í™•ì¸
            if self._is_question_completed(reasoning_type, i):
                result = self._load_completed_result(reasoning_type, i)
                results.append(result)
                self.logger.info(f"Loaded cached result for question {i + 1}/{len(dataset)}")
                continue
            
            # ìƒˆë¡œìš´ ì§ˆë¬¸ í‰ê°€
            result = self.evaluate_single_question(question_data, reasoning_type)
            results.append(result)
            
            # ì¤‘ê°„ ì €ì¥
            self._save_progress(reasoning_type, i, result)
            
            if (i + 1) % 5 == 0:
                self.logger.info(f"Completed {i + 1}/{len(dataset)} questions")
                # ì¤‘ê°„ í†µê³„ ì €ì¥
                stats = self._calculate_stats(results, reasoning_type)
                self._save_intermediate_results(reasoning_type, results, stats)
        
        # í†µê³„ ê³„ì‚°
        stats = self._calculate_stats(results, reasoning_type)
        return results, stats
    
    def _calculate_stats(
        self, 
        results: List[EvaluationResult], 
        reasoning_type: str
    ) -> BenchmarkStats:
        """í†µê³„ ê³„ì‚°"""
        total_questions = len(results)
        correct_answers = sum(1 for r in results if r.is_correct)
        accuracy = correct_answers / total_questions if total_questions > 0 else 0.0
        
        valid_times = [r.response_time for r in results if r.response_time > 0]
        avg_response_time = sum(valid_times) / len(valid_times) if valid_times else 0.0
        
        bridge_results = [r for r in results if r.question_type == "bridge"]
        comparison_results = [r for r in results if r.question_type == "comparison"]
        
        bridge_accuracy = (
            sum(1 for r in bridge_results if r.is_correct) / len(bridge_results)
            if bridge_results else 0.0
        )
        comparison_accuracy = (
            sum(1 for r in comparison_results if r.is_correct) / len(comparison_results)
            if comparison_results else 0.0
        )
        
        return BenchmarkStats(
            reasoning_type=reasoning_type,
            total_questions=total_questions,
            correct_answers=correct_answers,
            accuracy=accuracy,
            avg_response_time=avg_response_time,
            bridge_accuracy=bridge_accuracy,
            comparison_accuracy=comparison_accuracy,
            bridge_count=len(bridge_results),
            comparison_count=len(comparison_results)
        )
    
    def _setup_result_files(self):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ íŒŒì¼ ê²½ë¡œ ì„¤ì •"""
        self.detailed_file = self.results_dir / f"hotpotqa_detailed_results_{self.timestamp}.json"
        self.summary_file = self.results_dir / f"hotpotqa_summary_{self.timestamp}.json"
        self.progress_file = self.results_dir / f"hotpotqa_progress_{self.timestamp}.json"
        
        # ì§„í–‰ ìƒí™© íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë¡œë“œ
        self.completed_questions = {}
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    self.completed_questions = json.load(f)
                self.logger.info(f"Loaded progress from {self.progress_file}")
                self.logger.info(f"Previously completed: {sum(len(v) for v in self.completed_questions.values())} questions")
            except Exception as e:
                self.logger.warning(f"Could not load progress file: {e}")
                self.completed_questions = {}
    
    def _save_progress(self, reasoning_type: str, question_idx: int, result: EvaluationResult):
        """ì§„í–‰ ìƒí™©ì„ ì¤‘ê°„ ì €ì¥"""
        if reasoning_type not in self.completed_questions:
            self.completed_questions[reasoning_type] = {}
        
        self.completed_questions[reasoning_type][str(question_idx)] = {
            "question": result.question,
            "expected_answer": result.expected_answer,
            "predicted_answer": result.predicted_answer,
            "is_correct": result.is_correct,
            "response_time": result.response_time,
            "question_type": result.question_type,
            "error": result.error
        }
        
        # ì§„í–‰ ìƒí™© ì €ì¥
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.completed_questions, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.warning(f"Could not save progress: {e}")
    
    def _is_question_completed(self, reasoning_type: str, question_idx: int) -> bool:
        """íŠ¹ì • ì§ˆë¬¸ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return (reasoning_type in self.completed_questions and 
                str(question_idx) in self.completed_questions[reasoning_type])
    
    def _load_completed_result(self, reasoning_type: str, question_idx: int) -> EvaluationResult:
        """ì™„ë£Œëœ ê²°ê³¼ ë¡œë“œ"""
        data = self.completed_questions[reasoning_type][str(question_idx)]
        return EvaluationResult(
            reasoning_type=reasoning_type,
            question=data["question"],
            expected_answer=data["expected_answer"],
            predicted_answer=data["predicted_answer"],
            is_correct=data["is_correct"],
            response_time=data["response_time"],
            question_type=data["question_type"],
            error=data.get("error")
        )

    def save_results(
        self, 
        all_results: Dict[str, List[EvaluationResult]], 
        all_stats: Dict[str, BenchmarkStats]
    ):
        """ê²°ê³¼ ì €ì¥"""
        # self.timestamp ì‚¬ìš© (ì´ˆê¸°í™”ì—ì„œ ì„¤ì •ë¨)
        
        # ìƒì„¸ ê²°ê³¼
        detailed_results = {}
        for reasoning_type, results in all_results.items():
            detailed_results[reasoning_type] = [
                {
                    "question": r.question,
                    "expected_answer": r.expected_answer,
                    "predicted_answer": r.predicted_answer,
                    "is_correct": r.is_correct,
                    "response_time": r.response_time,
                    "question_type": r.question_type,
                    "error": r.error
                }
                for r in results
            ]
        
        # ì´ë¯¸ ì„¤ì •ëœ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©
        with open(self.detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        # í†µê³„ ìš”ì•½
        summary_stats = {}
        for reasoning_type, stats in all_stats.items():
            summary_stats[reasoning_type] = {
                "total_questions": stats.total_questions,
                "correct_answers": stats.correct_answers,
                "accuracy": stats.accuracy,
                "avg_response_time": stats.avg_response_time,
                "bridge_accuracy": stats.bridge_accuracy,
                "comparison_accuracy": stats.comparison_accuracy,
                "bridge_count": stats.bridge_count,
                "comparison_count": stats.comparison_count
            }
        
        with open(self.summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_stats, f, indent=2, ensure_ascii=False)
        
        print(f"Results saved to {self.detailed_file} and {self.summary_file}")
    
    def _save_intermediate_results(
        self, 
        reasoning_type: str, 
        results: List[EvaluationResult], 
        stats: BenchmarkStats
    ):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        try:
            # í˜„ì¬ê¹Œì§€ì˜ ìƒì„¸ ê²°ê³¼
            detailed_results = {
                reasoning_type: [
                    {
                        "question": r.question,
                        "expected_answer": r.expected_answer,
                        "predicted_answer": r.predicted_answer,
                        "is_correct": r.is_correct,
                        "response_time": r.response_time,
                        "question_type": r.question_type,
                        "error": r.error
                    }
                    for r in results
                ]
            }
            
            # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œí•˜ì—¬ í•©ì¹˜ê¸°
            if self.detailed_file.exists():
                with open(self.detailed_file, 'r', encoding='utf-8') as f:
                    existing_results = json.load(f)
                existing_results.update(detailed_results)
                detailed_results = existing_results
            
            # ìƒì„¸ ê²°ê³¼ ì €ì¥
            with open(self.detailed_file, 'w', encoding='utf-8') as f:
                json.dump(detailed_results, f, indent=2, ensure_ascii=False)
            
            # í†µê³„ ìš”ì•½ ì €ì¥
            summary_stats = {
                reasoning_type: {
                    "total_questions": stats.total_questions,
                    "correct_answers": stats.correct_answers,
                    "accuracy": stats.accuracy,
                    "avg_response_time": stats.avg_response_time,
                    "bridge_accuracy": stats.bridge_accuracy,
                    "comparison_accuracy": stats.comparison_accuracy,
                    "bridge_count": stats.bridge_count,
                    "comparison_count": stats.comparison_count
                }
            }
            
            # ê¸°ì¡´ ìš”ì•½ì´ ìˆìœ¼ë©´ ë¡œë“œí•˜ì—¬ í•©ì¹˜ê¸°
            if self.summary_file.exists():
                with open(self.summary_file, 'r', encoding='utf-8') as f:
                    existing_summary = json.load(f)
                existing_summary.update(summary_stats)
                summary_stats = existing_summary
            
            # ìš”ì•½ ì €ì¥
            with open(self.summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_stats, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            self.logger.warning(f"Could not save intermediate results: {e}")

    def run_evaluation(self) -> Dict[str, BenchmarkStats]:
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        print("ğŸš€ Starting HotpotQA benchmark evaluation...")
        print(f"ğŸ“ Results will be saved with timestamp: {self.timestamp}")
        
        # ì´ì „ ì§„í–‰ ìƒí™©ì´ ìˆëŠ” ê²½ìš° ì¶œë ¥
        if self.completed_questions:
            total_completed = sum(len(v) for v in self.completed_questions.values())
            print(f"ğŸ“Š Found {total_completed} previously completed questions")
            for reasoning_type, questions in self.completed_questions.items():
                print(f"   - {reasoning_type}: {len(questions)} questions")
        
        dataset = self.load_dataset()
        all_results = {}
        all_stats = {}
        
        for reasoning_type in ["cot"]:#, "react", "reflexion"]:
            print(f"\n{'='*50}")
            print(f"Evaluating {reasoning_type.upper()} reasoning")
            print(f"{'='*50}")
            
            results, stats = self.evaluate_reasoning_type(dataset, reasoning_type)
            all_results[reasoning_type] = results
            all_stats[reasoning_type] = stats
            
            self._print_stats(stats)
            # ê° reasoning type ì™„ë£Œ í›„ ì¤‘ê°„ ì €ì¥
            self._save_intermediate_results(reasoning_type, results, stats)
        
        self.save_results(all_results, all_stats)
        self._print_comparison(all_stats)
        
        return all_stats
    
    def _print_stats(self, stats: BenchmarkStats):
        """í†µê³„ ì¶œë ¥"""
        print(f"\n{stats.reasoning_type.upper()} Results:")
        print(f"  Total Questions: {stats.total_questions}")
        print(f"  Correct Answers: {stats.correct_answers}")
        print(f"  Overall Accuracy: {stats.accuracy:.3f}")
        print(f"  Bridge Questions Accuracy: {stats.bridge_accuracy:.3f} ({stats.bridge_count} questions)")
        print(f"  Comparison Questions Accuracy: {stats.comparison_accuracy:.3f} ({stats.comparison_count} questions)")
        print(f"  Average Response Time: {stats.avg_response_time:.2f}s")
    
    def _print_comparison(self, all_stats: Dict[str, BenchmarkStats]):
        """ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("FINAL COMPARISON")
        print(f"{'='*60}")
        
        print(f"{'Method':<12} {'Accuracy':<10} {'Bridge':<10} {'Comparison':<12} {'Avg Time':<10}")
        print("-" * 60)
        
        for reasoning_type, stats in all_stats.items():
            print(f"{reasoning_type.upper():<12} {stats.accuracy:<10.3f} {stats.bridge_accuracy:<10.3f} "
                  f"{stats.comparison_accuracy:<12.3f} {stats.avg_response_time:<10.2f}s")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Simple HotpotQA Benchmark Evaluation")
    parser.add_argument("--max-samples", type=int, default=10, help="Maximum number of samples")
    parser.add_argument("--model", type=str, default="openai:gpt-4o-mini", help="Model to use")
    parser.add_argument("--timestamp", type=str, help="Timestamp for continuing previous evaluation (format: YYYYMMDD_HHMMSS)")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducible sampling")
    
    args = parser.parse_args()
    
    print(f"ğŸ§ª HotpotQA Evaluation with {args.max_samples} questions")
    if args.timestamp:
        print(f"ğŸ“ Using timestamp: {args.timestamp} (continuing previous evaluation)")
    print(f"ğŸ² Using seed: {args.seed}")
    print("=" * 60)
    
    evaluator = SimpleHotpotQAEvaluator(
        max_samples=args.max_samples,
        model=args.model,
        timestamp=args.timestamp,
        seed=args.seed
    )
    
    try:
        evaluator.run_evaluation()
        print("\nğŸ‰ Evaluation completed successfully!")
    except KeyboardInterrupt:
        print("\nEvaluation interrupted by user")
        print("ğŸ’¾ Progress has been saved. You can continue with the same timestamp.")
    except Exception as e:
        print(f"Error during evaluation: {e}")
        print("ğŸ’¾ Progress has been saved. You can continue with the same timestamp.")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
