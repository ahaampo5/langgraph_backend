#!/usr/bin/env python3
"""
HotpotQA í‰ê°€ê¸° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from evaluate.benchmark_test.hotpotqa.evaluator import HotpotQAEvaluator


def test_evaluator_creation():
    """í‰ê°€ê¸° ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("=== Testing Evaluator Creation ===")
    
    try:
        evaluator = HotpotQAEvaluator(
            max_samples=5,  # 5ê°œ ìƒ˜í”Œë§Œ í…ŒìŠ¤íŠ¸
            parallel_workers=1  # ë‹¨ì¼ ì›Œì»¤ë¡œ í…ŒìŠ¤íŠ¸
        )
        print("âœ“ Evaluator created successfully")
        return evaluator
    except Exception as e:
        print(f"âŒ Error creating evaluator: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_dataset_loading(evaluator):
    """ë°ì´í„°ì…‹ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\n=== Testing Dataset Loading ===")
    
    try:
        dataset = evaluator.load_dataset()
        print(f"âœ“ Dataset loaded successfully: {len(dataset)} questions")
        
        # ì²« ë²ˆì§¸ ì§ˆë¬¸ ì¶œë ¥
        if dataset:
            first_q = dataset[0]
            print(f"  Sample question: {first_q['question']}")
            print(f"  Answer: {first_q['answer']}")
            print(f"  Type: {first_q['type']}")
        
        return dataset
    except Exception as e:
        print(f"âŒ Error loading dataset: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_single_evaluation(evaluator, dataset):
    """ë‹¨ì¼ ì§ˆë¬¸ í‰ê°€ í…ŒìŠ¤íŠ¸"""
    print("\n=== Testing Single Question Evaluation ===")
    
    if not dataset:
        print("âŒ No dataset available for testing")
        return False
    
    try:
        # ì²« ë²ˆì§¸ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        question_data = dataset[0]
        print(f"Testing question: {question_data['question']}")
        
        # CoT ë°©ë²•ë¡ ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        result = evaluator.evaluate_single_question(question_data, "cot")
        
        print(f"âœ“ Single evaluation completed")
        print(f"  Expected: {result.expected_answer}")
        print(f"  Predicted: {result.predicted_answer}")
        print(f"  Correct: {result.is_correct}")
        print(f"  Time: {result.response_time:.2f}s")
        
        return True
    except Exception as e:
        print(f"âŒ Error in single evaluation: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_answer_extraction(evaluator):
    """ë‹µë³€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    print("\n=== Testing Answer Extraction ===")
    
    test_responses = [
        ("Final Answer: Paris", "bridge"),
        ("The answer is yes", "comparison"),
        ("Based on my analysis, the answer is New York", "bridge"),
        ("No, they are not the same", "comparison")
    ]
    
    try:
        for response, q_type in test_responses:
            extracted = evaluator.extract_final_answer(response, q_type)
            print(f"  '{response}' -> '{extracted}'")
        
        print("âœ“ Answer extraction test completed")
        return True
    except Exception as e:
        print(f"âŒ Error in answer extraction: {e}")
        return False


def test_mini_evaluation(evaluator):
    """ì†Œê·œëª¨ í‰ê°€ í…ŒìŠ¤íŠ¸ (3ê°œ ì§ˆë¬¸ë§Œ)"""
    print("\n=== Testing Mini Evaluation ===")
    
    try:
        # ë§¤ìš° ì‘ì€ ìƒ˜í”Œë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
        evaluator.max_samples = 3
        dataset = evaluator.load_dataset()
        
        if not dataset:
            print("âŒ No dataset for mini evaluation")
            return False
        
        # CoT ë°©ë²•ë¡ ìœ¼ë¡œë§Œ í…ŒìŠ¤íŠ¸
        results, stats = evaluator.evaluate_reasoning_type(dataset, "cot")
        
        print(f"âœ“ Mini evaluation completed")
        print(f"  Evaluated {len(results)} questions")
        print(f"  Accuracy: {stats.accuracy:.3f}")
        print(f"  Average time: {stats.avg_response_time:.2f}s")
        
        return True
    except Exception as e:
        print(f"âŒ Error in mini evaluation: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª Starting HotpotQA Evaluator Test Suite")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    success_count = 0
    total_tests = 5
    
    # 1. í‰ê°€ê¸° ìƒì„± í…ŒìŠ¤íŠ¸
    evaluator = test_evaluator_creation()
    if evaluator:
        success_count += 1
    else:
        print("âŒ Cannot proceed without evaluator")
        return
    
    # 2. ë°ì´í„°ì…‹ ë¡œë”© í…ŒìŠ¤íŠ¸
    dataset = test_dataset_loading(evaluator)
    if dataset:
        success_count += 1
    
    # 3. ë‹µë³€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    if test_answer_extraction(evaluator):
        success_count += 1
    
    # 4. ë‹¨ì¼ í‰ê°€ í…ŒìŠ¤íŠ¸
    if test_single_evaluation(evaluator, dataset):
        success_count += 1
    
    # 5. ì†Œê·œëª¨ í‰ê°€ í…ŒìŠ¤íŠ¸
    if test_mini_evaluation(evaluator):
        success_count += 1
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print(f"Test Results: {success_count}/{total_tests} tests passed")
    
    if success_count == total_tests:
        print("ğŸ‰ All tests passed! The evaluator is ready to use.")
        print("\nTo run a full evaluation:")
        print("python evaluate/benchmark_test/hotpotqa/evaluator.py --max-samples 50")
    else:
        print("âŒ Some tests failed. Please check the errors above.")
    
    print("\nğŸ‘‹ Test suite completed.")


if __name__ == "__main__":
    main()
