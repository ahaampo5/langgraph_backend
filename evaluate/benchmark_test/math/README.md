# Math Benchmark Evaluation

ì´ ë””ë ‰í† ë¦¬ëŠ” EleutherAI/hendrycks_math ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ CoT, ReAct, Reflexion ìˆ˜í•™ ì—ì´ì „íŠ¸ë“¤ì„ í‰ê°€í•˜ëŠ” ì½”ë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ“‹ ë°ì´í„°ì…‹ ì •ë³´

**EleutherAI/hendrycks_math** ë°ì´í„°ì…‹ì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜í•™ ì˜ì—­ì„ í¬í•¨í•©ë‹ˆë‹¤:

- `algebra`: ëŒ€ìˆ˜í•™
- `counting_and_probability`: ì¡°í•©ë¡  ë° í™•ë¥ 
- `geometry`: ê¸°í•˜í•™  
- `intermediate_algebra`: ì¤‘ê¸‰ ëŒ€ìˆ˜í•™
- `number_theory`: ì •ìˆ˜ë¡ 
- `prealgebra`: ê¸°ì´ˆ ëŒ€ìˆ˜í•™
- `precalculus`: ë¯¸ì ë¶„í•™ ì˜ˆë¹„ê³¼ì •

ê° ë°ì´í„° í¬ì¸íŠ¸ëŠ” ë‹¤ìŒ ì»¬ëŸ¼ì„ í¬í•¨í•©ë‹ˆë‹¤:
- `problem`: ìˆ˜í•™ ë¬¸ì œ í…ìŠ¤íŠ¸
- `level`: ë‚œì´ë„ (1-5, 5ê°€ ê°€ì¥ ì–´ë ¤ì›€)
- `type`: ìˆ˜í•™ ì˜ì—­ (ì˜ˆ: "Algebra")
- `solution`: ì •ë‹µ í’€ì´ ê³¼ì •

## ğŸ—‚ï¸ íŒŒì¼ êµ¬ì¡°

```
evaluate/benchmark_test/math/
â”œâ”€â”€ evaluate.py           # ë©”ì¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ demo_evaluation.py    # ê°„ë‹¨í•œ ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ test_evaluation.py    # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ README.md            # ì´ íŒŒì¼
```

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. í™˜ê²½ ì„¤ì •

```bash
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install datasets transformers

# ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /Users/admin/Desktop/workspace/my_github/langgraph_service
```

### 2. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸

```bash
# í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
cd evaluate/benchmark_test/math
python test_evaluation.py
```

### 3. ë°ëª¨ ì‹¤í–‰

```bash
# ëª‡ ê°œì˜ ë¬¸ì œë¡œ ê°„ë‹¨í•œ ë°ëª¨
python demo_evaluation.py
```

### 4. ì „ì²´ í‰ê°€ ì‹¤í–‰

```bash
# ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ì „ì²´ í‰ê°€
python evaluate.py
```

## ğŸ“Š í‰ê°€ ê²°ê³¼

í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

### ì „ì²´ ì„±ëŠ¥
- ê° ì—ì´ì „íŠ¸ë³„ ì •í™•ë„
- í‰ê·  ì‹¤í–‰ ì‹œê°„
- ì˜¤ë¥˜ìœ¨

### ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
- ìˆ˜í•™ ì˜ì—­ë³„ ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¹„êµ
- ê° ì˜ì—­ì—ì„œì˜ ì •í™•ë„

### ë‚œì´ë„ë³„ ì„±ëŠ¥  
- ë ˆë²¨ 1-5ë³„ ì„±ëŠ¥ ë¶„ì„
- ë‚œì´ë„ì— ë”°ë¥¸ ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë³€í™”

### ì˜ˆì‹œ ì¶œë ¥

```
ğŸ“Š MATH BENCHMARK EVALUATION REPORT
================================================================================
ğŸ¯ OVERALL RESULTS
----------------------------------------
COT Agent:
  ğŸ“ˆ Accuracy: 67.50%
  âœ… Correct: 27/40
  â±ï¸  Avg Time: 8.45s
  âŒ Errors: 2

REACT Agent:
  ğŸ“ˆ Accuracy: 72.50%
  âœ… Correct: 29/40
  â±ï¸  Avg Time: 12.30s
  âŒ Errors: 1

REFLEXION Agent:
  ğŸ“ˆ Accuracy: 75.00%
  âœ… Correct: 30/40
  â±ï¸  Avg Time: 15.60s
  âŒ Errors: 0

ğŸ† Best Overall: REFLEXION Agent (75.00%)

ğŸ“š RESULTS BY CATEGORY
----------------------------------------
Algebra:
  COT: 70.00% (7/10)
  REACT: 80.00% (8/10)
  REFLEXION: 80.00% (8/10)

Geometry:
  COT: 60.00% (6/10)
  REACT: 70.00% (7/10)
  REFLEXION: 75.00% (7.5/10)
```

## âš™ï¸ ì„¤ì • ì˜µì…˜

### evaluate.py ë§¤ê°œë³€ìˆ˜

```python
# ì¹´í…Œê³ ë¦¬ë³„ ìµœëŒ€ ë¬¸ì œ ìˆ˜ ì¡°ì •
max_problems_per_category = 20  # ê¸°ë³¸ê°’: 20

# í‰ê°€í•  ì—ì´ì „íŠ¸ íƒ€ì… ì„ íƒ
agent_types = ["cot", "react", "reflexion"]  # ë˜ëŠ” ì¼ë¶€ë§Œ ì„ íƒ

# ëª¨ë¸ ë³€ê²½
model = "openai:gpt-4o-mini"  # ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸
```

### ì‚¬ìš©ì ì •ì˜ í‰ê°€

```python
from evaluate import MathBenchmarkEvaluator

# í‰ê°€ê¸° ìƒì„±
evaluator = MathBenchmarkEvaluator(model="openai:gpt-4o-mini")

# íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í‰ê°€
results = evaluator.evaluate_category("algebra", max_problems=10)

# ê²°ê³¼ ë¶„ì„
report = evaluator.generate_report({"algebra": results})
evaluator.print_report(report)
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ì˜¤ë¥˜

1. **ImportError: math_agents**
   ```bash
   # ì˜¬ë°”ë¥¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”
   cd /Users/admin/Desktop/workspace/my_github/langgraph_service
   ```

2. **Dataset loading error**
   ```bash
   # datasets íŒ¨í‚¤ì§€ ì„¤ì¹˜
   pip install datasets transformers
   ```

3. **API í‚¤ ì˜¤ë¥˜**
   ```bash
   # .env íŒŒì¼ì— OpenAI API í‚¤ ì„¤ì •
   echo "OPENAI_API_KEY=your_key_here" > .env
   ```

### ì„±ëŠ¥ ìµœì í™”

- `max_problems_per_category`ë¥¼ ì¤„ì—¬ì„œ ë” ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
- íŠ¹ì • ì—ì´ì „íŠ¸ë§Œ í‰ê°€í•˜ì—¬ ì‹œê°„ ë‹¨ì¶•
- ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© (ì˜ˆ: gpt-3.5-turbo)

## ğŸ“ˆ ê²°ê³¼ ë¶„ì„

### ì˜ˆìƒë˜ëŠ” ì„±ëŠ¥ íŒ¨í„´

1. **Reflexion > ReAct > CoT**: ì¼ë°˜ì ìœ¼ë¡œ ìê¸° ë°˜ì„±ì´ í¬í•¨ëœ Reflexionì´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥
2. **ë ˆë²¨ë³„ ì„±ëŠ¥ ì €í•˜**: ë‚œì´ë„ê°€ ë†’ì•„ì§ˆìˆ˜ë¡ ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ ì €í•˜
3. **ì˜ì—­ë³„ ì°¨ì´**: ê¸°í•˜í•™ì´ ëŒ€ìˆ˜í•™ë³´ë‹¤ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ

### ê²°ê³¼ í•´ì„

- **ì •í™•ë„**: ë¬¸ì œë¥¼ ì˜¬ë°”ë¥´ê²Œ í•´ê²°í•œ ë¹„ìœ¨
- **ì‹¤í–‰ ì‹œê°„**: í‰ê·  ë¬¸ì œ í•´ê²° ì‹œê°„ (Reflexionì´ ê°€ì¥ ì˜¤ë˜ ê±¸ë¦¼)
- **ì˜¤ë¥˜ìœ¨**: ì‹¤í–‰ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜ ë¹„ìœ¨

## ğŸ”„ í™•ì¥ ë°©ë²•

### ìƒˆë¡œìš´ í‰ê°€ ë©”íŠ¸ë¦­ ì¶”ê°€

```python
def calculate_partial_credit(agent_answer: str, solution: str) -> float:
    """ë¶€ë¶„ ì ìˆ˜ ê³„ì‚°"""
    # êµ¬í˜„...
    pass
```

### ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ ì¶”ê°€

```python
def evaluate_custom_dataset(self, dataset_name: str):
    """ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ í‰ê°€"""
    # êµ¬í˜„...
    pass
```

### ê²°ê³¼ ì‹œê°í™”

```python
import matplotlib.pyplot as plt

def plot_results(report):
    """ê²°ê³¼ ì‹œê°í™”"""
    # êµ¬í˜„...
    pass
```

## ğŸ“ ì°¸ê³  ìë£Œ

- [EleutherAI/hendrycks_math Dataset](https://huggingface.co/datasets/EleutherAI/hendrycks_math)
- [LangGraph Documentation](https://python.langchain.com/docs/langgraph)
- [Math Agents Implementation](../../../graphs/agent/experiments/)
