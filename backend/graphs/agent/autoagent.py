# filepath: /Users/admin/Desktop/workspace/my_github/langgraph_backend/backend/graphs/agent/autoagent.py

"""
ê³„íš ê¸°ë°˜ ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ (AutoAgent)
- ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê³„íšì„ ìˆ˜ë¦½
- ê° ë‹¨ê³„ë³„ë¡œ LLMì´ ì‹¤í–‰
- ì›¹ ê²€ìƒ‰, íŒŒì¼ ì‹œìŠ¤í…œ, MCP ë„êµ¬ í™œìš©
- ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìš”ì•½ ê¸°ëŠ¥
"""

from typing import Annotated, Any, List, Dict, Optional, Literal
from typing_extensions import TypedDict
from pydantic import BaseModel, Field

# LangGraph imports
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt.chat_agent_executor import AgentState
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.types import Command

# LangChain imports
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage, AnyMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langchain_tavily import TavilySearch
from langchain_core.messages.utils import trim_messages, count_tokens_approximately

# MCP imports
from langchain_mcp_adapters.client import MultiServerMCPClient

import os
import json
import asyncio
from pathlib import Path


class PlanStep(BaseModel):
    """ê³„íšì˜ ê° ë‹¨ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤"""
    step_id: int
    description: str
    status: Literal["pending", "in_progress", "completed", "failed"] = "pending"
    result: Optional[str] = None
    tools_needed: List[str] = []


class Plan(BaseModel):
    """ì „ì²´ ê³„íšì„ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤"""
    goal: str
    steps: List[PlanStep]
    current_step: int = 0
    status: Literal["planning", "executing", "completed", "failed"] = "planning"


class AutoAgentState(AgentState):
    """AutoAgent ì—ì´ì „íŠ¸ì˜ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    messages: Annotated[list[AnyMessage], add_messages]
    plan: Optional[Plan] = None
    current_step_result: Optional[str] = None
    context: Dict[str, Any] = {}
    user_query: str = ""
    final_answer: Optional[str] = None


# ë„êµ¬ ì •ì˜
@tool
def web_search(query: str, max_results: int = 5) -> str:
    """ì›¹ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    try:
        tavily = TavilySearch(max_results=max_results)
        results = tavily.run(query)
        return f"ì›¹ ê²€ìƒ‰ ê²°ê³¼ ('{query}'):\n{results}"
    except Exception as e:
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
def read_file(file_path: str) -> str:
    """íŒŒì¼ì˜ ë‚´ìš©ì„ ì½ìŠµë‹ˆë‹¤."""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        return f"íŒŒì¼ '{file_path}' ë‚´ìš©:\n{content}"
    except FileNotFoundError:
        return f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"
    except Exception as e:
        return f"íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool 
def write_file(file_path: str, content: str) -> str:
    """íŒŒì¼ì— ë‚´ìš©ì„ ì”ë‹ˆë‹¤."""
    try:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as file:
            file.write(content)
        return f"íŒŒì¼ '{file_path}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"íŒŒì¼ ì“°ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@tool
def list_directory(directory_path: str) -> str:
    """ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        path = Path(directory_path)
        if not path.exists():
            return f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory_path}"
        
        files = []
        for item in path.iterdir():
            if item.is_file():
                files.append(f"ğŸ“„ {item.name}")
            elif item.is_dir():
                files.append(f"ğŸ“ {item.name}/")
        
        return f"ë””ë ‰í† ë¦¬ '{directory_path}' ë‚´ìš©:\n" + "\n".join(files)
    except Exception as e:
        return f"ë””ë ‰í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# ê¸°ë³¸ ë„êµ¬ ëª©ë¡
TOOLS = [web_search, read_file, write_file, list_directory]


def create_planner_node(model: ChatOpenAI):
    """ê³„íš ìˆ˜ë¦½ ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    def planner(state: AutoAgentState, config: RunnableConfig) -> dict:
        """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤."""
        
        user_query = state["user_query"]
        if not user_query and state["messages"]:
            # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ì§ˆë¬¸ ì¶”ì¶œ
            for msg in reversed(state["messages"]):
                if isinstance(msg, HumanMessage):
                    user_query = msg.content
                    break
        
        planning_prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

ë‹¤ìŒ ë„êµ¬ë“¤ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- web_search: ì›¹ì—ì„œ ì •ë³´ ê²€ìƒ‰
- read_file: íŒŒì¼ ë‚´ìš© ì½ê¸°  
- write_file: íŒŒì¼ì— ë‚´ìš© ì“°ê¸°
- list_directory: ë””ë ‰í† ë¦¬ íŒŒì¼ ëª©ë¡ ì¡°íšŒ

ê³„íšì„ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
{{
    "goal": "ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ëª©í‘œ",
    "steps": [
        {{
            "step_id": 1,
            "description": "ì²« ë²ˆì§¸ ë‹¨ê³„ ì„¤ëª…",
            "tools_needed": ["í•„ìš”í•œ_ë„êµ¬1", "í•„ìš”í•œ_ë„êµ¬2"]
        }},
        {{
            "step_id": 2, 
            "description": "ë‘ ë²ˆì§¸ ë‹¨ê³„ ì„¤ëª…",
            "tools_needed": ["í•„ìš”í•œ_ë„êµ¬3"]
        }}
    ]
}}

ê° ë‹¨ê³„ëŠ” ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•˜ë©°, ë…¼ë¦¬ì  ìˆœì„œë¡œ ë°°ì—´ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."""
        
        messages = [SystemMessage(content=planning_prompt)]
        response = model.invoke(messages)
        
        try:
            # JSON ì¶”ì¶œ (```json ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì²˜ë¦¬)
            content = response.content
            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                json_str = content[json_start:json_end].strip()
            elif "{" in content:
                json_start = content.find("{")
                json_end = content.rfind("}") + 1
                json_str = content[json_start:json_end]
            else:
                raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            plan_data = json.loads(json_str)
            
            # Plan ê°ì²´ ìƒì„±
            steps = [PlanStep(**step) for step in plan_data["steps"]]
            plan = Plan(
                goal=plan_data["goal"],
                steps=steps,
                status="executing"
            )
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            ai_message = AIMessage(content=f"ê³„íšì„ ìˆ˜ë¦½í–ˆìŠµë‹ˆë‹¤:\nëª©í‘œ: {plan.goal}\në‹¨ê³„ ìˆ˜: {len(plan.steps)}")
            
            return {
                "messages": [ai_message],
                "plan": plan,
                "user_query": user_query
            }
            
        except Exception as e:
            error_msg = f"ê³„íš ìˆ˜ë¦½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            return {
                "messages": [AIMessage(content=error_msg)],
                "plan": None
            }
    
    return planner


def create_executor_node(model: ChatOpenAI, tools: List):
    """ê³„íš ì‹¤í–‰ ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    def executor(state: AutoAgentState, config: RunnableConfig) -> dict:
        """í˜„ì¬ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        plan = state["plan"]
        if not plan or plan.current_step >= len(plan.steps):
            return {"messages": [AIMessage(content="ì‹¤í–‰í•  ë‹¨ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.")]}
        
        current_step = plan.steps[plan.current_step]
        current_step.status = "in_progress"
        
        # ì‹¤í–‰ í”„ë¡¬í”„íŠ¸
        execution_prompt = f"""ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”:

ëª©í‘œ: {plan.goal}
í˜„ì¬ ë‹¨ê³„ ({current_step.step_id}/{len(plan.steps)}): {current_step.description}

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {', '.join(current_step.tools_needed) if current_step.tools_needed else 'ëª¨ë“  ë„êµ¬'}

ì´ì „ ë‹¨ê³„ë“¤ì˜ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ í˜„ì¬ ë‹¨ê³„ë¥¼ ì™„ë£Œí•´ì£¼ì„¸ìš”.
í•„ìš”í•œ ê²½ìš° ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê³ , ë‹¨ê³„ ì™„ë£Œ í›„ ê²°ê³¼ë¥¼ ëª…í™•íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
        
        # ëª¨ë¸ì— ë„êµ¬ ë°”ì¸ë”©
        model_with_tools = model.bind_tools(tools)
        
        messages = [SystemMessage(content=execution_prompt)] + state["messages"]
        response = model_with_tools.invoke(messages)
        
        # ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        if response.tool_calls:
            tool_node = ToolNode(tools)
            
            # ë„êµ¬ ì‹¤í–‰ì„ ìœ„í•œ ë©”ì‹œì§€ ì¤€ë¹„
            tool_state = {"messages": messages + [response]}
            tool_result = tool_node.invoke(tool_state)
            tool_messages = tool_result["messages"]
            
            # ë„êµ¬ ê²°ê³¼ë¥¼ í¬í•¨í•œ ìµœì¢… ì‘ë‹µ ìƒì„±
            final_messages = messages + [response] + tool_messages
            final_response = model.invoke(final_messages)
            
            current_step.result = final_response.content
            current_step.status = "completed"
            
            return {
                "messages": [response] + tool_messages + [final_response],
                "plan": plan,
                "current_step_result": final_response.content
            }
        else:
            current_step.result = response.content
            current_step.status = "completed"
            
            return {
                "messages": [response],
                "plan": plan,
                "current_step_result": response.content
            }
    
    return executor


def create_finalizer_node(model: ChatOpenAI):
    """ìµœì¢… ë‹µë³€ ìƒì„± ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    def finalizer(state: AutoAgentState, config: RunnableConfig) -> dict:
        """ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ í›„ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        
        plan = state["plan"]
        if not plan:
            return {"messages": [AIMessage(content="ê³„íšì´ ì—†ì–´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]}
        
        # ëª¨ë“  ë‹¨ê³„ ê²°ê³¼ ìˆ˜ì§‘
        step_results = []
        for i, step in enumerate(plan.steps, 1):
            step_results.append(f"ë‹¨ê³„ {i}: {step.description}\nê²°ê³¼: {step.result}")
        
        finalization_prompt = f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ì›ë˜ ì§ˆë¬¸: {state['user_query']}
ëª©í‘œ: {plan.goal}

ì‹¤í–‰í•œ ë‹¨ê³„ë“¤ê³¼ ê²°ê³¼:
{chr(10).join(step_results)}

ìœ„ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ì™„ì „í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        messages = [SystemMessage(content=finalization_prompt)]
        response = model.invoke(messages)
        
        return {
            "messages": [response],
            "final_answer": response.content
        }
    
    return finalizer


def create_summarization_node(model: ChatOpenAI):
    """ë©”ì‹œì§€ ìš”ì•½ ë…¸ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    def summarizer(state: AutoAgentState, config: RunnableConfig) -> dict:
        """ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ìš”ì•½í•©ë‹ˆë‹¤."""
        
        messages = state["messages"]
        
        # í† í° ìˆ˜ í™•ì¸ (ëŒ€ëµì )
        total_tokens = count_tokens_approximately(messages)
        
        if total_tokens > 8000:  # ì„ê³„ê°’
            # ìµœì‹  ë©”ì‹œì§€ë“¤ ìœ ì§€í•˜ë©´ì„œ ì´ì „ ë©”ì‹œì§€ë“¤ ìš”ì•½
            recent_messages = messages[-10:]  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ ìœ ì§€
            old_messages = messages[:-10]
            
            if old_messages:
                summary_prompt = """ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì¤‘ìš”í•œ ì •ë³´ì™€ ì»¨í…ìŠ¤íŠ¸ëŠ” ë³´ì¡´í•˜ë˜, ë¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ì€ ì œê±°í•´ì£¼ì„¸ìš”.

ëŒ€í™” ë‚´ìš©:""" + "\n".join([f"{msg.type}: {msg.content}" for msg in old_messages])
                
                summary_response = model.invoke([SystemMessage(content=summary_prompt)])
                summary_message = SystemMessage(
                    content=f"[ì´ì „ ëŒ€í™” ìš”ì•½] {summary_response.content}"
                )
                
                return {
                    "messages": [summary_message] + recent_messages
                }
        
        return {"messages": messages}
    
    return summarizer


def should_continue(state: AutoAgentState) -> str:
    """ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í• ì§€ ê²°ì •í•©ë‹ˆë‹¤."""
    
    plan = state["plan"]
    if not plan:
        return "end"
    
    if plan.current_step < len(plan.steps):
        # ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰
        plan.current_step += 1
        if plan.current_step < len(plan.steps):
            return "execute"
        else:
            # ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ
            plan.status = "completed"
            return "finalize"
    
    return "end"


def needs_summarization(state: AutoAgentState) -> str:
    """ë©”ì‹œì§€ ìš”ì•½ì´ í•„ìš”í•œì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    
    total_tokens = count_tokens_approximately(state["messages"])
    if total_tokens > 8000:
        return "summarize"
    return "continue"


class AutoAgent:
    """AutoAgent í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = "gpt-4o-mini", enable_mcp: bool = False):
        self.model = ChatOpenAI(model=model_name, temperature=0)
        self.tools = TOOLS.copy()
        self.enable_mcp = enable_mcp
        self.mcp_client = None
        
        # MCP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        if enable_mcp:
            self._setup_mcp()
        
        # ë©”ëª¨ë¦¬ ì„¤ì •
        self.checkpointer = MemorySaver()

        # ê·¸ë˜í”„ êµ¬ì„±
        self.graph = self._create_graph()
    
    def _setup_mcp(self):
        """MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        try:
            self.mcp_client = MultiServerMCPClient({
                "math": {
                    "command": "python",
                    "args": ["/Users/admin/Desktop/workspace/my_github/langgraph_backend/backend/mcp/math/math.py"],
                    "transport": "stdio",
                },
                "web_search": {
                    "url": "http://localhost:8931/mcp",
                    "transport": "streamable_http",
                },
                "playwright": {
                    "url": "http://localhost:8080/mcp",
                    "transport": "streamable_http",
                }
            })
        except Exception as e:
            print(f"MCP ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _get_mcp_tools(self):
        """MCP ë„êµ¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        if self.mcp_client:
            try:
                mcp_tools = await self.mcp_client.get_tools()
                return mcp_tools
            except Exception as e:
                print(f"MCP ë„êµ¬ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return []
    
    def _create_graph(self):
        """LangGraphë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
        
        # ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(AutoAgentState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("planner", create_planner_node(self.model))
        workflow.add_node("executor", create_executor_node(self.model, self.tools))
        workflow.add_node("finalizer", create_finalizer_node(self.model))
        workflow.add_node("summarizer", create_summarization_node(self.model))
        
        # ì‹œì‘ì  ì„¤ì •
        workflow.add_edge(START, "planner")
        
        # ì¡°ê±´ë¶€ ì—£ì§€ ì„¤ì •
        workflow.add_conditional_edges(
            "planner",
            should_continue,
            {
                "execute": "executor",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "executor", 
            should_continue,
            {
                "execute": "executor",
                "finalize": "finalizer", 
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "finalizer",
            needs_summarization,
            {
                "summarize": "summarizer",
                "continue": END
            }
        )
        
        workflow.add_edge("summarizer", END)
        
        return workflow.compile(checkpointer=self.checkpointer)
    
    async def ainvoke(self, user_input: str, thread_id: str = "default") -> dict:
        """ë¹„ë™ê¸°ë¡œ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        # MCP ë„êµ¬ ì¶”ê°€
        if self.enable_mcp:
            mcp_tools = await self._get_mcp_tools()
            self.tools.extend(mcp_tools)
        
        config = {
            "configurable": {
                "thread_id": thread_id
            }
        }
        
        initial_state = {
            "messages": [HumanMessage(content=user_input)],
            "user_query": user_input
        }
        
        result = await self.graph.ainvoke(initial_state, config)
        return result
    
    def invoke(self, user_input: str, thread_id: str = "default") -> dict:
        """ë™ê¸°ë¡œ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        config = {
            "configurable": {
                "thread_id": thread_id
            }
        }
        
        initial_state = {
            "messages": [HumanMessage(content=user_input)],
            "user_query": user_input
        }
        
        result = self.graph.invoke(initial_state, config)
        return result


# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """AutoAgent ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"""
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = AutoAgent(enable_mcp=True)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_queries = [
        # "ê¹€ì¹˜ ë ˆì‹œí”¼ë¥¼ ì°¾ì•„ì„œ ë‹¨ê³„ë³„ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”",
        # "í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ê³  README íŒŒì¼ì„ ì‘ì„±í•´ì£¼ì„¸ìš”",
        # "Pythonìœ¼ë¡œ ê°„ë‹¨í•œ ê³„ì‚°ê¸° í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”"
        "êµ¬ê¸€ ë“¤ì–´ê°€ì„œ LangGraph ê²€ìƒ‰í•˜ê³  githubì— ë“¤ì–´ê°€ì„œ README.md ì²« ì¤„ ì¶œë ¥í•´ì¤˜"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'='*50}")
        print(f"í…ŒìŠ¤íŠ¸ {i}: {query}")
        print('='*50)
        
        try:
            result = await agent.ainvoke(query, thread_id=f"test_{i}")
            
            print(f"ìµœì¢… ë‹µë³€: {result.get('final_answer', 'ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.')}")
            
            if result.get('plan'):
                print(f"\nì‹¤í–‰ëœ ê³„íš:")
                plan = result['plan']
                print(f"ëª©í‘œ: {plan.goal}")
                for step in plan.steps:
                    print(f"- ë‹¨ê³„ {step.step_id}: {step.description} ({step.status})")
                    
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    asyncio.run(main())